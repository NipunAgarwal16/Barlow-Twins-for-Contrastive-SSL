{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code File",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVFDeOz+2vaAsFPdVvCA3I"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "0bJTsy4_ntHv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoGL8zZOnrNd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# slightly faster improvements, on the first epoch 30 second decrease and a 1-2 second\n",
        "# decrease in epoch time. Overall saves approx. 5 min of training time\n",
        "\n",
        "# Allocates two threads for a gpu private which allows more operations to be\n",
        "# done faster\n",
        "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
        "\n",
        "import tensorflow as tf  # framework\n",
        "from tensorflow import keras  # for tf.keras\n",
        "import tensorflow_addons as tfa  # LAMB optimizer and gaussian_blur_2d function\n",
        "import numpy as np  # np.random.random\n",
        "import matplotlib.pyplot as plt  # graphs\n",
        "import datetime  # tensorboard logs naming\n",
        "\n",
        "# XLA optimization for faster performance(up to 10-15 minutes total time saved)\n",
        "tf.config.optimizer.set_jit(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading the CIFAR-10 dataset**"
      ],
      "metadata": {
        "id": "yqflr9e7nxrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[\n",
        "    (train_features, train_labels),\n",
        "    (test_features, test_labels),\n",
        "] = keras.datasets.cifar10.load_data()\n",
        "\n",
        "train_features = train_features / 255.0\n",
        "test_features = test_features / 255.0"
      ],
      "metadata": {
        "id": "SRk1piucnzJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining Necessary Hyperparameters**"
      ],
      "metadata": {
        "id": "x1DDDtFcn0vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size of dataset\n",
        "BATCH_SIZE = 512\n",
        "# Width and height of image\n",
        "IMAGE_SIZE = 32"
      ],
      "metadata": {
        "id": "lk8j_Zs9n5PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Augmentation Utilities**"
      ],
      "metadata": {
        "id": "n6cOQM05n7ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Augmentation(keras.layers.Layer):\n",
        "    \"\"\"Base augmentation class.\n",
        "\n",
        "    Base augmentation class. Contains the random_execute method.\n",
        "\n",
        "    Methods:\n",
        "        random_execute: method that returns true or false based\n",
        "          on a probability. Used to determine whether an augmentation\n",
        "          will be run.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Augmentation, self).__init__()\n",
        "\n",
        "    @tf.function\n",
        "    def random_execute(self, prob: float) -> bool:\n",
        "        \"\"\"random_execute function.\n",
        "\n",
        "        Arguments:\n",
        "            prob: a float value from 0-1 that determines the\n",
        "              probability.\n",
        "\n",
        "        Returns:\n",
        "            returns true or false based on the probability.\n",
        "        \"\"\"\n",
        "\n",
        "        return tf.random.uniform([], minval=0, maxval=1) < prob\n",
        "\n",
        "\n",
        "class RandomToGrayscale(Augmentation):\n",
        "    \"\"\"RandomToGrayscale class.\n",
        "\n",
        "    RandomToGrayscale class. Randomly makes an image\n",
        "    grayscaled based on the random_execute method. There\n",
        "    is a 20% chance that an image will be grayscaled.\n",
        "\n",
        "    Methods:\n",
        "        call: method that grayscales an image 20% of\n",
        "          the time.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a grayscaled version of the image 20% of the time\n",
        "              and the original image 80% of the time.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.random_execute(0.2):\n",
        "            x = tf.image.rgb_to_grayscale(x)\n",
        "            x = tf.tile(x, [1, 1, 3])\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomColorJitter(Augmentation):\n",
        "    \"\"\"RandomColorJitter class.\n",
        "\n",
        "    RandomColorJitter class. Randomly adds color jitter to an image.\n",
        "    Color jitter means to add random brightness, contrast,\n",
        "    saturation, and hue to an image. There is a 80% chance that an\n",
        "    image will be randomly color-jittered.\n",
        "\n",
        "    Methods:\n",
        "        call: method that color-jitters an image 80% of\n",
        "          the time.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Adds color jitter to image, including:\n",
        "          Brightness change by a max-delta of 0.8\n",
        "          Contrast change by a max-delta of 0.8\n",
        "          Saturation change by a max-delta of 0.8\n",
        "          Hue change by a max-delta of 0.2\n",
        "        Originally, the same deltas of the original paper\n",
        "        were used, but a performance boost of almost 2% was found\n",
        "        when doubling them.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a color-jittered version of the image 80% of the time\n",
        "              and the original image 20% of the time.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.random_execute(0.8):\n",
        "            x = tf.image.random_brightness(x, 0.8)\n",
        "            x = tf.image.random_contrast(x, 0.4, 1.6)\n",
        "            x = tf.image.random_saturation(x, 0.4, 1.6)\n",
        "            x = tf.image.random_hue(x, 0.2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomFlip(Augmentation):\n",
        "    \"\"\"RandomFlip class.\n",
        "\n",
        "    RandomFlip class. Randomly flips image horizontally. There is a 50%\n",
        "    chance that an image will be randomly flipped.\n",
        "\n",
        "    Methods:\n",
        "        call: method that flips an image 50% of\n",
        "          the time.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Randomly flips the image.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a flipped version of the image 50% of the time\n",
        "              and the original image 50% of the time.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.random_execute(0.5):\n",
        "            x = tf.image.random_flip_left_right(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomResizedCrop(Augmentation):\n",
        "    \"\"\"RandomResizedCrop class.\n",
        "\n",
        "    RandomResizedCrop class. Randomly crop an image to a random size,\n",
        "    then resize the image back to the original size.\n",
        "\n",
        "    Attributes:\n",
        "        image_size: The dimension of the image\n",
        "\n",
        "    Methods:\n",
        "        __call__: method that does random resize crop to the image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size):\n",
        "        super(Augmentation, self).__init__()\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Does random resize crop by randomly cropping an image to a random\n",
        "        size 75% - 100% the size of the image. Then resizes it.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a randomly cropped image.\n",
        "        \"\"\"\n",
        "\n",
        "        rand_size = tf.random.uniform(\n",
        "            shape=[],\n",
        "            minval=int(0.75 * self.image_size),\n",
        "            maxval=1 * self.image_size,\n",
        "            dtype=tf.int32,\n",
        "        )\n",
        "\n",
        "        crop = tf.image.random_crop(x, (rand_size, rand_size, 3))\n",
        "        crop_resize = tf.image.resize(crop, (self.image_size, self.image_size))\n",
        "        return crop_resize\n",
        "\n",
        "\n",
        "class RandomSolarize(Augmentation):\n",
        "    \"\"\"RandomSolarize class.\n",
        "\n",
        "    RandomSolarize class. Randomly solarizes an image.\n",
        "    Solarization is when pixels accidentally flip to an inverted state.\n",
        "\n",
        "    Methods:\n",
        "        call: method that does random solarization 20% of the time.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Randomly solarizes the image.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a solarized version of the image 20% of the time\n",
        "              and the original image 80% of the time.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.random_execute(0.2):\n",
        "            # flips abnormally low pixels to abnormally high pixels\n",
        "            x = tf.where(x < 10, x, 255 - x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomBlur(Augmentation):\n",
        "    \"\"\"RandomBlur class.\n",
        "\n",
        "    RandomBlur class. Randomly blurs an image.\n",
        "\n",
        "    Methods:\n",
        "        call: method that does random blur 20% of the time.\n",
        "    \"\"\"\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call function.\n",
        "\n",
        "        Randomly solarizes the image.\n",
        "\n",
        "        Arguments:\n",
        "            x: a tf.Tensor representing the image.\n",
        "\n",
        "        Returns:\n",
        "            returns a blurred version of the image 20% of the time\n",
        "              and the original image 80% of the time.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.random_execute(0.2):\n",
        "            s = np.random.random()\n",
        "            return tfa.image.gaussian_filter2d(image=x, sigma=s)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RandomAugmentor(keras.Model):\n",
        "    \"\"\"RandomAugmentor class.\n",
        "\n",
        "    RandomAugmentor class. Chains all the augmentations into\n",
        "    one pipeline.\n",
        "\n",
        "    Attributes:\n",
        "        image_size: An integer represing the width and height\n",
        "          of the image. Designed to be used for square images.\n",
        "        random_resized_crop: Instance variable representing the\n",
        "          RandomResizedCrop layer.\n",
        "        random_flip: Instance variable representing the\n",
        "          RandomFlip layer.\n",
        "        random_color_jitter: Instance variable representing the\n",
        "          RandomColorJitter layer.\n",
        "        random_blur: Instance variable representing the\n",
        "          RandomBlur layer\n",
        "        random_to_grayscale: Instance variable representing the\n",
        "          RandomToGrayscale layer\n",
        "        random_solarize: Instance variable representing the\n",
        "          RandomSolarize layer\n",
        "\n",
        "    Methods:\n",
        "        call: chains layers in pipeline together\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size: int):\n",
        "        super(RandomAugmentor, self).__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.random_resized_crop = RandomResizedCrop(image_size)\n",
        "        self.random_flip = RandomFlip()\n",
        "        self.random_color_jitter = RandomColorJitter()\n",
        "        self.random_blur = RandomBlur()\n",
        "        self.random_to_grayscale = RandomToGrayscale()\n",
        "        self.random_solarize = RandomSolarize()\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        x = self.random_resized_crop(x)\n",
        "        x = self.random_flip(x)\n",
        "        x = self.random_color_jitter(x)\n",
        "        x = self.random_blur(x)\n",
        "        x = self.random_to_grayscale(x)\n",
        "        x = self.random_solarize(x)\n",
        "\n",
        "        x = tf.clip_by_value(x, 0, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "bt_augmentor = RandomAugmentor(IMAGE_SIZE)"
      ],
      "metadata": {
        "id": "pXYE1PTsn8UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Data**"
      ],
      "metadata": {
        "id": "geQiBFREoB2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BTDatasetCreator:\n",
        "    \"\"\"Barlow twins dataset creator class.\n",
        "\n",
        "    BTDatasetCreator class. Responsible for creating the\n",
        "    barlow twins' dataset.\n",
        "\n",
        "    Attributes:\n",
        "        options: tf.data.Options needed to configure a setting\n",
        "          that may improve performance.\n",
        "        seed: random seed for shuffling. Used to synchronize two\n",
        "          augmented versions.\n",
        "        augmentor: augmentor used for augmentation.\n",
        "\n",
        "    Methods:\n",
        "        __call__: creates barlow dataset.\n",
        "        augmented_version: creates 1 half of the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, augmentor: RandomAugmentor, seed: int = 1024):\n",
        "        self.options = tf.data.Options()\n",
        "        self.options.threading.max_intra_op_parallelism = 1\n",
        "        self.seed = seed\n",
        "        self.augmentor = augmentor\n",
        "\n",
        "    def augmented_version(self, ds: list) -> tf.data.Dataset:\n",
        "        return (\n",
        "            tf.data.Dataset.from_tensor_slices(ds)\n",
        "            .shuffle(1000, seed=self.seed)\n",
        "            .map(self.augmentor, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            .batch(BATCH_SIZE, drop_remainder=True)\n",
        "            .prefetch(tf.data.AUTOTUNE)\n",
        "            .with_options(self.options)\n",
        "        )\n",
        "\n",
        "    def __call__(self, ds: list) -> tf.data.Dataset:\n",
        "        a1 = self.augmented_version(ds)\n",
        "        a2 = self.augmented_version(ds)\n",
        "\n",
        "        return tf.data.Dataset.zip((a1, a2)).with_options(self.options)\n",
        "\n",
        "\n",
        "augment_versions = BTDatasetCreator(bt_augmentor)(train_features)"
      ],
      "metadata": {
        "id": "Ys_oUzQBoE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Viewing examples from Dataset**"
      ],
      "metadata": {
        "id": "YQqMl-6GoI_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_augment_versions = iter(augment_versions)\n",
        "\n",
        "\n",
        "def plot_values(batch: tuple):\n",
        "    fig, axs = plt.subplots(3, 3)\n",
        "    fig1, axs1 = plt.subplots(3, 3)\n",
        "\n",
        "    fig.suptitle(\"Augmentation 1\")\n",
        "    fig1.suptitle(\"Augmentation 2\")\n",
        "\n",
        "    a1, a2 = batch\n",
        "\n",
        "    # plots images on both tables\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            # CHANGE(add / 255)\n",
        "            axs[i][j].imshow(a1[3 * i + j])\n",
        "            axs[i][j].axis(\"off\")\n",
        "            axs1[i][j].imshow(a2[3 * i + j])\n",
        "            axs1[i][j].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_values(next(sample_augment_versions))"
      ],
      "metadata": {
        "id": "bCjXXBFNoOGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Barlow twins model's loss function**"
      ],
      "metadata": {
        "id": "3UkMWaR3oRgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BarlowLoss(keras.losses.Loss):\n",
        "    \"\"\"BarlowLoss class.\n",
        "\n",
        "    BarlowLoss class. Creates a loss function based on the cross-correlation\n",
        "    matrix.\n",
        "\n",
        "    Attributes:\n",
        "        batch_size: the batch size of the dataset\n",
        "        lambda_amt: the value for lambda(used in cross_corr_matrix_loss)\n",
        "\n",
        "    Methods:\n",
        "        __init__: gets instance variables\n",
        "        call: gets the loss based on the cross-correlation matrix\n",
        "          make_diag_zeros: Used in calculating off-diagonal section\n",
        "          of loss function; makes diagonals zeros.\n",
        "        cross_corr_matrix_loss: creates loss based on cross correlation\n",
        "          matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size: int):\n",
        "        \"\"\"__init__ method.\n",
        "\n",
        "        Gets the instance variables\n",
        "\n",
        "        Arguments:\n",
        "            batch_size: An integer value representing the batch size of the\n",
        "              dataset. Used for cross correlation matrix calculation.\n",
        "        \"\"\"\n",
        "\n",
        "        super(BarlowLoss, self).__init__()\n",
        "        self.lambda_amt = 5e-3\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def get_off_diag(self, c: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"get_off_diag method.\n",
        "\n",
        "        Makes the diagonals of the cross correlation matrix zeros.\n",
        "        This is used in the off-diagonal portion of the loss function,\n",
        "        where we take the squares of the off-diagonal values and sum them.\n",
        "\n",
        "        Arguments:\n",
        "            c: A tf.tensor that represents the cross correlation\n",
        "              matrix\n",
        "\n",
        "        Returns:\n",
        "            Returns a tf.tensor which represents the cross correlation\n",
        "            matrix with its diagonals as zeros.\n",
        "        \"\"\"\n",
        "\n",
        "        zero_diag = tf.zeros(c.shape[-1])\n",
        "        return tf.linalg.set_diag(c, zero_diag)\n",
        "\n",
        "    def cross_corr_matrix_loss(self, c: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"cross_corr_matrix_loss method.\n",
        "\n",
        "        Gets the loss based on the cross correlation matrix.\n",
        "        We want the diagonals to be 1's and everything else to be\n",
        "        zeros to show that the two augmented images are similar.\n",
        "\n",
        "        Loss function procedure:\n",
        "        take the diagonal of the cross-correlation matrix, subtract by 1,\n",
        "        and square that value so no negatives.\n",
        "\n",
        "        Take the off-diagonal of the cc-matrix(see get_off_diag()),\n",
        "        square those values to get rid of negatives and increase the value,\n",
        "        and multiply it by a lambda to weight it such that it is of equal\n",
        "        value to the optimizer as the diagonal(there are more values off-diag\n",
        "        then on-diag)\n",
        "\n",
        "        Take the sum of the first and second parts and then sum them together.\n",
        "\n",
        "        Arguments:\n",
        "            c: A tf.tensor that represents the cross correlation\n",
        "              matrix\n",
        "\n",
        "        Returns:\n",
        "            Returns a tf.tensor which represents the cross correlation\n",
        "            matrix with its diagonals as zeros.\n",
        "        \"\"\"\n",
        "\n",
        "        # subtracts diagonals by one and squares them(first part)\n",
        "        c_diff = tf.pow(tf.linalg.diag_part(c) - 1, 2)\n",
        "\n",
        "        # takes off diagonal, squares it, multiplies with lambda(second part)\n",
        "        off_diag = tf.pow(self.get_off_diag(c), 2) * self.lambda_amt\n",
        "\n",
        "        # sum first and second parts together\n",
        "        loss = tf.reduce_sum(c_diff) + tf.reduce_sum(off_diag)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def normalize(self, output: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"normalize method.\n",
        "\n",
        "        Normalizes the model prediction.\n",
        "\n",
        "        Arguments:\n",
        "            output: the model prediction.\n",
        "\n",
        "        Returns:\n",
        "            Returns a normalized version of the model prediction.\n",
        "        \"\"\"\n",
        "\n",
        "        return (output - tf.reduce_mean(output, axis=0)) / tf.math.reduce_std(\n",
        "            output, axis=0\n",
        "        )\n",
        "\n",
        "    def cross_corr_matrix(self, z_a_norm: tf.Tensor, z_b_norm: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"cross_corr_matrix method.\n",
        "\n",
        "        Creates a cross correlation matrix from the predictions.\n",
        "        It transposes the first prediction and multiplies this with\n",
        "        the second, creating a matrix with shape (n_dense_units, n_dense_units).\n",
        "        See build_twin() for more info. Then it divides this with the\n",
        "        batch size.\n",
        "\n",
        "        Arguments:\n",
        "            z_a_norm: A normalized version of the first prediction.\n",
        "            z_b_norm: A normalized version of the second prediction.\n",
        "\n",
        "        Returns:\n",
        "            Returns a cross correlation matrix.\n",
        "        \"\"\"\n",
        "        return (tf.transpose(z_a_norm) @ z_b_norm) / self.batch_size\n",
        "\n",
        "    def call(self, z_a: tf.Tensor, z_b: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"call method.\n",
        "\n",
        "        Makes the cross-correlation loss. Uses the CreateCrossCorr\n",
        "        class to make the cross corr matrix, then finds the loss and\n",
        "        returns it(see cross_corr_matrix_loss()).\n",
        "\n",
        "        Arguments:\n",
        "            z_a: The prediction of the first set of augmented data.\n",
        "            z_b: the prediction of the second set of augmented data.\n",
        "\n",
        "        Returns:\n",
        "            Returns a (rank-0) tf.Tensor that represents the loss.\n",
        "        \"\"\"\n",
        "\n",
        "        z_a_norm, z_b_norm = self.normalize(z_a), self.normalize(z_b)\n",
        "        c = self.cross_corr_matrix(z_a_norm, z_b_norm)\n",
        "        loss = self.cross_corr_matrix_loss(c)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "fVn55gjdoS2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Barlow Twins' Model Architecture**"
      ],
      "metadata": {
        "id": "Oj9IZmXVoWHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet34:\n",
        "    \"\"\"Resnet34 class.\n",
        "\n",
        "        Responsible for the Resnet 34 architecture.\n",
        "    Modified from\n",
        "    https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/#h2_2.\n",
        "    https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/#h2_2.\n",
        "        View their website for more information.\n",
        "    \"\"\"\n",
        "\n",
        "    def identity_block(self, x, filter):\n",
        "        # copy tensor to variable called x_skip\n",
        "        x_skip = x\n",
        "        # Layer 1\n",
        "        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        # Layer 2\n",
        "        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "        # Add Residue\n",
        "        x = tf.keras.layers.Add()([x, x_skip])\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        return x\n",
        "\n",
        "    def convolutional_block(self, x, filter):\n",
        "        # copy tensor to variable called x_skip\n",
        "        x_skip = x\n",
        "        # Layer 1\n",
        "        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\", strides=(2, 2))(x)\n",
        "        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        # Layer 2\n",
        "        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "        # Processing Residue with conv(1,1)\n",
        "        x_skip = tf.keras.layers.Conv2D(filter, (1, 1), strides=(2, 2))(x_skip)\n",
        "        # Add Residue\n",
        "        x = tf.keras.layers.Add()([x, x_skip])\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        return x\n",
        "\n",
        "    def __call__(self, shape=(32, 32, 3)):\n",
        "        # Step 1 (Setup Input Layer)\n",
        "        x_input = tf.keras.layers.Input(shape)\n",
        "        x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
        "        # Step 2 (Initial Conv layer along with maxPool)\n",
        "        x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\"same\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "        x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
        "        # Define size of sub-blocks and initial filter size\n",
        "        block_layers = [3, 4, 6, 3]\n",
        "        filter_size = 64\n",
        "        # Step 3 Add the Resnet Blocks\n",
        "        for i in range(4):\n",
        "            if i == 0:\n",
        "                # For sub-block 1 Residual/Convolutional block not needed\n",
        "                for j in range(block_layers[i]):\n",
        "                    x = self.identity_block(x, filter_size)\n",
        "            else:\n",
        "                # One Residual/Convolutional Block followed by Identity blocks\n",
        "                # The filter size will go on increasing by a factor of 2\n",
        "                filter_size = filter_size * 2\n",
        "                x = self.convolutional_block(x, filter_size)\n",
        "                for j in range(block_layers[i] - 1):\n",
        "                    x = self.identity_block(x, filter_size)\n",
        "        # Step 4 End Dense Network\n",
        "        x = tf.keras.layers.AveragePooling2D((2, 2), padding=\"same\")(x)\n",
        "        x = tf.keras.layers.Flatten()(x)\n",
        "        model = tf.keras.models.Model(inputs=x_input, outputs=x, name=\"ResNet34\")\n",
        "        return model"
      ],
      "metadata": {
        "id": "gXQcBF3coXw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Projector network**"
      ],
      "metadata": {
        "id": "B89jkszpobfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_twin() -> keras.Model:\n",
        "    \"\"\"build_twin method.\n",
        "\n",
        "    Builds a barlow twins model consisting of an encoder(resnet-34)\n",
        "    and a projector, which generates embeddings for the images\n",
        "\n",
        "    Returns:\n",
        "        returns a barlow twins model\n",
        "    \"\"\"\n",
        "\n",
        "    # number of dense neurons in the projector\n",
        "    n_dense_neurons = 5000\n",
        "\n",
        "    # encoder network\n",
        "    resnet = ResNet34()()\n",
        "    last_layer = resnet.layers[-1].output\n",
        "\n",
        "    # intermediate layers of the projector network\n",
        "    n_layers = 2\n",
        "    for i in range(n_layers):\n",
        "        dense = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{i}\")\n",
        "        if i == 0:\n",
        "            x = dense(last_layer)\n",
        "        else:\n",
        "            x = dense(x)\n",
        "        x = tf.keras.layers.BatchNormalization(name=f\"projector_bn_{i}\")(x)\n",
        "        x = tf.keras.layers.ReLU(name=f\"projector_relu_{i}\")(x)\n",
        "\n",
        "    x = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{n_layers}\")(x)\n",
        "\n",
        "    model = keras.Model(resnet.input, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "u9o1_OIDodJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Loop Model**"
      ],
      "metadata": {
        "id": "SvnCkERsoftw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BarlowModel(keras.Model):\n",
        "    \"\"\"BarlowModel class.\n",
        "\n",
        "    BarlowModel class. Responsible for making predictions and handling\n",
        "    gradient descent with the optimizer.\n",
        "\n",
        "    Attributes:\n",
        "        model: the barlow model architecture.\n",
        "        loss_tracker: the loss metric.\n",
        "\n",
        "    Methods:\n",
        "        train_step: one train step; do model predictions, loss, and\n",
        "            optimizer step.\n",
        "        metrics: Returns metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BarlowModel, self).__init__()\n",
        "        self.model = build_twin()\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def train_step(self, batch: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"train_step method.\n",
        "\n",
        "        Do one train step. Make model predictions, find loss, pass loss to\n",
        "        optimizer, and make optimizer apply gradients.\n",
        "\n",
        "        Arguments:\n",
        "            batch: one batch of data to be given to the loss function.\n",
        "\n",
        "        Returns:\n",
        "            Returns a dictionary with the loss metric.\n",
        "        \"\"\"\n",
        "\n",
        "        # get the two augmentations from the batch\n",
        "        y_a, y_b = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # get two versions of predictions\n",
        "            z_a, z_b = self.model(y_a, training=True), self.model(y_b, training=True)\n",
        "            loss = self.loss(z_a, z_b)\n",
        "\n",
        "        grads_model = tape.gradient(loss, self.model.trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads_model, self.model.trainable_variables))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "metadata": {
        "id": "q9emoyo5oiLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "_aL7rYHsolB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sets up model, optimizer, loss\n",
        "\n",
        "bm = BarlowModel()\n",
        "# chose the LAMB optimizer due to high batch sizes. Converged MUCH faster\n",
        "# than ADAM or SGD\n",
        "optimizer = tfa.optimizers.LAMB()\n",
        "loss = BarlowLoss(BATCH_SIZE)\n",
        "\n",
        "bm.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "# Expected training time: 1 hours 30 min\n",
        "\n",
        "history = bm.fit(augment_versions, epochs=160)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7MG0I0V4onGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating the Model**"
      ],
      "metadata": {
        "id": "yiGiTLnVopRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approx: 64% accuracy with this barlow twins model.\n",
        "\n",
        "xy_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "    .shuffle(1000)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((test_features, test_labels))\n",
        "    .shuffle(1000)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "model = keras.models.Sequential(\n",
        "    [\n",
        "        bm.model,\n",
        "        keras.layers.Dense(\n",
        "            10, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.02)\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "linear_optimizer = tfa.optimizers.LAMB()\n",
        "model.compile(\n",
        "    optimizer=linear_optimizer,\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(xy_ds, epochs=35, validation_data=test_ds)"
      ],
      "metadata": {
        "id": "vZVpFZekosDp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}